{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f469a609",
   "metadata": {},
   "source": [
    "This notebook contains the entiretly of the process of pre-processing, which includes the archi.. used form Data uplodation to storing to out weaviate vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e917c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epein5/Desktop/ADVANCED-RAG/.venv/lib/python3.12/site-packages/PyPDF2/__init__.py:21: DeprecationWarning: PyPDF2 is deprecated. Please move to the pypdf library instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epein5/Desktop/ADVANCED-RAG/.venv/lib/python3.12/site-packages/PyPDF2/__init__.py:21: DeprecationWarning: PyPDF2 is deprecated. Please move to the pypdf library instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43daa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Load data from TXT or PDF files with page tracking.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the TXT or PDF file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (full_text, list of (text, page_num) tuples)\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    if file_path.suffix.lower() == '.txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            return text, [(text, 1)]\n",
    "    \n",
    "    elif file_path.suffix.lower() == '.pdf':\n",
    "        full_text = \"\"\n",
    "        pages_data = []\n",
    "        \n",
    "        with open(file_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            for page_num, page in enumerate(pdf_reader.pages, start=1):\n",
    "                page_text = page.extract_text()\n",
    "                full_text += page_text\n",
    "                pages_data.append((page_text, page_num))\n",
    "        \n",
    "        return full_text, pages_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path.suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5ff855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text by removing extra whitespace, newlines, and normalizing spaces.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = '\\n'.join(line.strip() for line in text.split('\\n') if line.strip())\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Optional: Replace newlines with spaces for more compact chunks\n",
    "    # text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "749e76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 334378 characters\n",
      "Mapped 343982 character positions to pages\n"
     ]
    }
   ],
   "source": [
    "loaded_data, pages_data = load_data(\"../dummy_data/nepal_constiution.pdf\")\n",
    "\n",
    "# Build character position to page number mapping BEFORE any truncation or cleaning\n",
    "# This ensures we can map chunk positions back to original PDF pages\n",
    "char_to_page = {}\n",
    "current_pos = 0\n",
    "for page_text, page_num in pages_data:\n",
    "    for i in range(len(page_text)):\n",
    "        char_to_page[current_pos + i] = page_num\n",
    "    current_pos += len(page_text)\n",
    "\n",
    "# Now truncate and clean the data\n",
    "loaded_data = loaded_data\n",
    "loaded_data = clean_text(loaded_data)\n",
    "\n",
    "print(f\"Loaded {len(loaded_data)} characters\")\n",
    "print(f\"Mapped {len(char_to_page)} character positions to pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def recursive_splitter(text, max_chunk_size=1500, separators=None):\n",
    "    \"\"\"\n",
    "    Splits text hierarchically without overlaps.\n",
    "    Returns: List of chunks (text) and List of span annotations (start, end).\n",
    "    \"\"\"\n",
    "    if separators is None:\n",
    "        # Hierarchy: Double Newline -> Single Newline -> Sentence End -> Space -> Character\n",
    "        separators = [\"\\n\\n\", \"\\n\", r\"(?<=[.!?])\\s+\", \" \", \"\"]\n",
    "\n",
    "    def split_recursive(input_text, current_separators, offset=0):\n",
    "        # Base Case: Text is small enough or no more separators to try\n",
    "        if len(input_text) <= max_chunk_size or not current_separators:\n",
    "            return [input_text], [(offset, offset + len(input_text))]\n",
    "\n",
    "        # Select the highest priority separator\n",
    "        sep = current_separators[0]\n",
    "        remaining_seps = current_separators[1:]\n",
    "\n",
    "        # Split the text by the separator\n",
    "        if sep == \"\": # Final fallback to character-by-character if needed\n",
    "            raw_splits = list(input_text)\n",
    "        elif sep.startswith(\"(?<=\"): # Regex for sentence boundaries\n",
    "            raw_splits = re.split(sep, input_text)\n",
    "        else:\n",
    "            raw_splits = input_text.split(sep)\n",
    "\n",
    "        final_chunks = []\n",
    "        final_spans = []\n",
    "        \n",
    "        current_buffer = \"\"\n",
    "        current_offset = offset\n",
    "\n",
    "        for part in raw_splits:\n",
    "            # If we used a separator, re-add it (except for the last part)\n",
    "            # This ensures we don't lose punctuation or newlines\n",
    "            if sep != \"\" and not sep.startswith(\"(?<=\") and part != raw_splits[-1]:\n",
    "                part += sep\n",
    "            \n",
    "            # Check if this single part is ALREADY too big\n",
    "            if len(part) > max_chunk_size:\n",
    "                # If we have a buffer, flush it first\n",
    "                if current_buffer:\n",
    "                    final_chunks.append(current_buffer)\n",
    "                    final_spans.append((current_offset, current_offset + len(current_buffer)))\n",
    "                    current_offset += len(current_buffer)\n",
    "                    current_buffer = \"\"\n",
    "                \n",
    "                # Recursively split the \"too big\" part using the NEXT separator\n",
    "                sub_chunks, sub_spans = split_recursive(part, remaining_seps, current_offset)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "                final_spans.extend(sub_spans)\n",
    "                current_offset = sub_spans[-1][1]\n",
    "            \n",
    "            # If adding this part exceeds max_size, flush the buffer\n",
    "            elif len(current_buffer) + len(part) > max_chunk_size:\n",
    "                final_chunks.append(current_buffer)\n",
    "                final_spans.append((current_offset, current_offset + len(current_buffer)))\n",
    "                current_offset += len(current_buffer)\n",
    "                current_buffer = part\n",
    "            \n",
    "            else:\n",
    "                current_buffer += part\n",
    "\n",
    "        # Final flush for whatever is left in the buffer\n",
    "        if current_buffer:\n",
    "            final_chunks.append(current_buffer)\n",
    "            final_spans.append((current_offset, current_offset + len(current_buffer)))\n",
    "\n",
    "        return final_chunks, final_spans\n",
    "\n",
    "    return split_recursive(text, separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6dcffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = recursive_splitter(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17103f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_page(span_start, char_to_page):\n",
    "    \"\"\"\n",
    "    Get the original PDF page number for a chunk based on its starting position.\n",
    "    \n",
    "    Args:\n",
    "        span_start: Starting character position of the chunk in the document\n",
    "        char_to_page: Dictionary mapping character positions to original PDF page numbers\n",
    "        \n",
    "    Returns:\n",
    "        int: Original PDF page number (1-indexed)\n",
    "    \"\"\"\n",
    "    # Look up the exact position first\n",
    "    if span_start in char_to_page:\n",
    "        return char_to_page[span_start]\n",
    "    \n",
    "    # If exact position not found, find the nearest position before span_start\n",
    "    positions = sorted(char_to_page.keys())\n",
    "    for pos in reversed(positions):\n",
    "        if pos < span_start:\n",
    "            return char_to_page[pos]\n",
    "    \n",
    "    # Default to page 1 if position is before all mapped positions\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c2f09",
   "metadata": {},
   "source": [
    "## Contextual Retrieval with Gemini + Prompt Caching\n",
    "\n",
    "Generate concise context for each chunk using Google Gemini with prompt caching enabled.\n",
    "This explains where each chunk sits within the whole document to improve retrieval accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da7172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ChunkResponse(BaseModel):\n",
    "    context: str = Field(description=\"Concise context explaining where this chunk sits within the document.\")\n",
    "    chunk_type: str = Field(description=\"Classification or type of the chunk (e.g., 'section', 'definition').\")\n",
    "    breadcrumb: str = Field(description=\"Hierarchical path or breadcrumb showing chunk location in document structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072d1bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.genai as genai\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def generate_context_for_chunk(client, cache_name, chunk_text, chunk_idx=None, model_name=\"gemini-2.5-flash\"):\n",
    "    \"\"\"\n",
    "    Generate context for a single chunk using Gemini with prompt caching.\n",
    "    \n",
    "    Args:\n",
    "        client: Google Generative AI client\n",
    "        cache_name: Name of the cached content\n",
    "        chunk_text: Individual chunk text\n",
    "        chunk_idx: Optional chunk index for error messages\n",
    "        model_name: Model to use\n",
    "        \n",
    "    Returns:\n",
    "        ChunkResponse: Pydantic model with context, chunk_type, and breadcrumb\n",
    "    \"\"\"\n",
    "    chunk_prompt = f\"\"\"Here is the chunk to analyze:\n",
    "<chunk>\n",
    "{chunk_text}\n",
    "</chunk>\n",
    "\n",
    "Provide context for this chunk.\"\"\"\n",
    "    \n",
    "    idx_str = f\"Chunk {chunk_idx}: \" if chunk_idx is not None else \"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=chunk_prompt,\n",
    "            config=genai.types.GenerateContentConfig(\n",
    "                cached_content=cache_name,\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=ChunkResponse.model_json_schema(),\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=2048,\n",
    "                safety_settings=[\n",
    "                    genai.types.SafetySetting(\n",
    "                        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                        threshold=\"BLOCK_NONE\"\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract cache metrics\n",
    "        cache_read_tokens = getattr(response.usage_metadata, 'cache_read_input_tokens', 0) or 0\n",
    "        \n",
    "        # Check if response was truncated\n",
    "        if hasattr(response, 'candidates') and response.candidates:\n",
    "            finish_reason = getattr(response.candidates[0], 'finish_reason', None)\n",
    "            if finish_reason and str(finish_reason) == 'MAX_TOKENS':\n",
    "                print(f\"{idx_str}Response truncated\")\n",
    "                return None\n",
    "        \n",
    "        # Parse JSON response\n",
    "        if response.text:\n",
    "            text = response.text.strip()\n",
    "            if text.startswith('```json'):\n",
    "                text = text[7:]\n",
    "            if text.startswith('```'):\n",
    "                text = text[3:]\n",
    "            if text.endswith('```'):\n",
    "                text = text[:-3]\n",
    "            text = text.strip()\n",
    "            \n",
    "            data = json.loads(text)\n",
    "            result = ChunkResponse(**data)\n",
    "            result._cache_read_tokens = cache_read_tokens\n",
    "            return result\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"{idx_str}JSON parse error: {str(e)[:50]}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"{idx_str}Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332da1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_data(idx, chunk_text, llm_context_response, start, end, page_num):\n",
    "    \"\"\"\n",
    "    Create a chunk data dictionary with page information.\n",
    "    \n",
    "    Args:\n",
    "        idx: Chunk index\n",
    "        chunk_text: Original chunk text\n",
    "        context: ChunkResponse object with context, chunk_type, and breadcrumb\n",
    "        start: Span start\n",
    "        end: Span end\n",
    "        page_num: Page number\n",
    "        \n",
    "    Returns:\n",
    "        dict: Chunk data with context and page info\n",
    "    \"\"\"\n",
    "    # Extract fields from ChunkResponse object\n",
    "    context_text = llm_context_response.context if llm_context_response else \"\"\n",
    "    chunk_type = llm_context_response.chunk_type if llm_context_response else \"\"\n",
    "    breadcrumb = llm_context_response.breadcrumb if llm_context_response else \"\"\n",
    "    \n",
    "    contextualized_chunk = f\"{context_text}\\n\\n{chunk_text}\" if context_text else chunk_text\n",
    "    \n",
    "    return {\n",
    "        \"index\": idx,\n",
    "        \"page\": page_num,\n",
    "        \"original_chunk\": chunk_text,\n",
    "        \"context\": context_text,\n",
    "        \"chunk_type\": chunk_type,\n",
    "        \"breadcrumb\": breadcrumb,\n",
    "        \"contextualized_chunk\": contextualized_chunk,\n",
    "        \"span\": (start, end)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf4c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_contexts(\n",
    "    document_text, \n",
    "    chunks_with_spans, \n",
    "    char_to_page, \n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    cache_ttl_hours=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate context for all chunks using prompt caching.\n",
    "    \n",
    "    Args:\n",
    "        document_text: The full document text\n",
    "        chunks_with_spans: Tuple of (chunks_list, spans_list)\n",
    "        char_to_page: Dictionary mapping character positions to PDF page numbers\n",
    "        model_name: Google Generative AI model to use\n",
    "        cache_ttl_hours: Cache time-to-live in hours\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts with chunk data and context\n",
    "    \"\"\"\n",
    "    chunks_list, spans_list = chunks_with_spans\n",
    "    client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    chunk_data = []\n",
    "    \n",
    "    system_instruction = \"\"\"Generate retrieval context for the following chunk. The context should:\n",
    "- Be 2-3 sentences that situate this chunk within the document\n",
    "- Include key terms/topics from surrounding sections\n",
    "- Explain the chunk's purpose or what question it answers\n",
    "- Be self-contained (assume the reader only sees the chunk + your context)\"\"\"\n",
    "    \n",
    "    # Create cache with document + system instructions\n",
    "    print(\"Creating prompt cache...\")\n",
    "    try:\n",
    "        cache_doc_text = f\"<document>\\n{document_text}\\n</document>\\n\\nI will ask you to analyze chunks from this document.\"\n",
    "        \n",
    "        cache = client.caches.create(\n",
    "            model=model_name,\n",
    "            config=genai.types.CreateCachedContentConfig(\n",
    "                display_name=\"document_context_cache\",\n",
    "                system_instruction=system_instruction,\n",
    "                contents=[cache_doc_text],\n",
    "                ttl=f\"{int(cache_ttl_hours * 3600)}s\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        cache_name = cache.name\n",
    "        cache_tokens = getattr(cache.usage_metadata, 'total_token_count', 0) or 0\n",
    "        print(f\"Cache created: {cache_tokens:,} tokens cached\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating cache: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Process chunks\n",
    "    print(f\"Processing {len(chunks_list)} chunks...\")\n",
    "    total_cache_reads = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for idx, (chunk_text, (start, end)) in enumerate(\n",
    "        tqdm(zip(chunks_list, spans_list), total=len(chunks_list), desc=\"Chunks\")\n",
    "    ):\n",
    "        try:\n",
    "            response = generate_context_for_chunk(\n",
    "                client, cache_name, chunk_text, chunk_idx=idx, model_name=model_name\n",
    "            )\n",
    "            \n",
    "            if response is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            page_num = get_chunk_page(start, char_to_page)\n",
    "            chunk_item = create_chunk_data(idx, chunk_text, response, start, end, page_num)\n",
    "            chunk_data.append(chunk_item)\n",
    "            \n",
    "            # Track cache usage\n",
    "            cache_reads = getattr(response, '_cache_read_tokens', 0)\n",
    "            total_cache_reads += cache_reads\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Chunk {idx}: Error - {e}\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nProcessed: {len(chunk_data)}/{len(chunks_list)} chunks\")\n",
    "    if total_cache_reads > 0:\n",
    "        print(f\"Cache reads: {total_cache_reads:,} tokens\")\n",
    "    if failed_count > 0:\n",
    "        print(f\"Failed: {failed_count} chunks\")\n",
    "    \n",
    "    # Cleanup\n",
    "    try:\n",
    "        client.caches.delete(name=cache_name)\n",
    "        print(\"Cache cleaned up\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not delete cache: {e}\")\n",
    "    \n",
    "    return chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6463115",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_chunk_contexts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate contexts for all chunks with original PDF page numbers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m chunk_contexts = \u001b[43mgenerate_chunk_contexts\u001b[49m(loaded_data, chunks, char_to_page)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_chunk_contexts' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate contexts for all chunks with original PDF page numbers\n",
    "chunk_contexts = generate_chunk_contexts(loaded_data, chunks, char_to_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f835ebf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunk_contexts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Display sample chunk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mchunk_contexts\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_contexts:\n\u001b[32m      5\u001b[39m     sample = chunk_contexts[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'chunk_contexts' is not defined"
     ]
    }
   ],
   "source": [
    "# Display sample chunk\n",
    "print(f\"Total chunks: {len(chunk_contexts)}\")\n",
    "\n",
    "if chunk_contexts:\n",
    "    sample = chunk_contexts[0]\n",
    "    print(f\"\\nSample (Index 0):\")\n",
    "    print(f\"Original: {sample['original_chunk'][:200]}...\")\n",
    "    print(f\"Page: {sample['page']}, Type: {sample['chunk_type']}, Path: {sample['breadcrumb']}\")\n",
    "    print(f\"Context: {sample['context']}\")\n",
    "    print(f\"\\nContextualized: {sample['contextualized_chunk'][:300]}...\")\n",
    "\n",
    "print(\"\\nReady for Weaviate upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d8df7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02e12d",
   "metadata": {},
   "source": [
    "## Use OpenAI's embeddings to make embeddings\n",
    "\n",
    "Generate enbeddings out of contextualized_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4ef9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 13/13 [00:19<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 252 embeddings\n",
      "Embedding dimension: 3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Azure OpenAI Embeddings Configuration\n",
    "endpoint = \"https://grow-me82mm7z-eastus2.cognitiveservices.azure.com/\"\n",
    "deployment = \"text-embedding-3-large\"\n",
    "\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def generate_embeddings(texts: list[str], batch_size: int = 20) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Azure OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        batch_size: Number of texts to embed per API call\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        response = embedding_client.embeddings.create(\n",
    "            input=batch,\n",
    "            model=deployment\n",
    "        )\n",
    "        # Sort by index to maintain order\n",
    "        batch_embeddings = [item.embedding for item in sorted(response.data, key=lambda x: x.index)]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Generate embeddings for all contextualized chunks\n",
    "contextualized_texts = [chunk[\"contextualized_chunk\"] for chunk in chunk_contexts]\n",
    "embeddings = generate_embeddings(contextualized_texts)\n",
    "\n",
    "# Add embeddings to chunk_contexts\n",
    "for chunk, embedding in zip(chunk_contexts, embeddings):\n",
    "    chunk[\"vector\"] = embedding\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4804e7",
   "metadata": {},
   "source": [
    "## Prepare Data for Weaviate Upload\n",
    "\n",
    "Convert the contextualized chunks to Weaviate-ready format with embeddings and metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0846b923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Weaviate: True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Connect to Weaviate\n",
    "client = weaviate.connect_to_local(host=\"localhost\", port=8080)\n",
    "print(f\"Connected to Weaviate: {client.is_ready()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f433c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing Document collection\n"
     ]
    }
   ],
   "source": [
    "# # Delete existing collection if it exists (fresh start)\n",
    "# try:\n",
    "#     client.collections.delete(\"Document\")\n",
    "#     print(\"Deleted existing Document collection\")\n",
    "# except Exception as e:\n",
    "#     print(f\"No existing collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0106100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document collection created\n"
     ]
    }
   ],
   "source": [
    "# Create Document collection schema\n",
    "schema = {\n",
    "    \"class\": \"Document\",\n",
    "    \"vectorizer\": \"none\",  # We provide our own vectors\n",
    "    \"properties\": [\n",
    "        {\"name\": \"content\", \"dataType\": [\"text\"], \"description\": \"Original chunk text\"},\n",
    "        {\"name\": \"context\", \"dataType\": [\"text\"], \"description\": \"LLM-generated context\"},\n",
    "        {\"name\": \"contextualized_chunk\", \"dataType\": [\"text\"], \"description\": \"Context + content combined\"},\n",
    "        {\"name\": \"chunk_type\", \"dataType\": [\"string\"], \"description\": \"Type of chunk\"},\n",
    "        {\"name\": \"breadcrumb\", \"dataType\": [\"string\"], \"description\": \"Document path\"},\n",
    "        {\"name\": \"source\", \"dataType\": [\"string\"], \"description\": \"Source file\"},\n",
    "        {\"name\": \"chunk_id\", \"dataType\": [\"int\"], \"description\": \"Chunk index\"},\n",
    "        {\"name\": \"page_number\", \"dataType\": [\"int\"], \"description\": \"PDF page number\"},\n",
    "        {\"name\": \"created_at\", \"dataType\": [\"date\"], \"description\": \"Upload timestamp\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.collections.create_from_dict(schema)\n",
    "print(\"Document collection created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "842c16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to Weaviate: 100%|██████████| 252/252 [00:03<00:00, 72.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 252 chunks to Weaviate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload chunks with vectors to Weaviate\n",
    "documents = client.collections.get(\"Document\")\n",
    "current_time = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "for chunk in tqdm(chunk_contexts, desc=\"Uploading to Weaviate\"):\n",
    "    documents.data.insert(\n",
    "        properties={\n",
    "            \"content\": chunk[\"original_chunk\"],\n",
    "            \"context\": chunk[\"context\"],\n",
    "            \"contextualized_chunk\": chunk[\"contextualized_chunk\"],\n",
    "            \"chunk_type\": chunk[\"chunk_type\"],\n",
    "            \"breadcrumb\": chunk[\"breadcrumb\"],\n",
    "            \"source\": \"nepal_constitution.pdf\",\n",
    "            \"chunk_id\": chunk[\"index\"],\n",
    "            \"page_number\": chunk[\"page\"],\n",
    "            \"created_at\": current_time,\n",
    "        },\n",
    "        vector=chunk[\"vector\"]\n",
    "    )\n",
    "\n",
    "print(f\"Uploaded {len(chunk_contexts)} chunks to Weaviate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e356aea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in Weaviate: 252\n",
      "\n",
      "Sample documents:\n",
      "Chunk 203 (Page 119):\n",
      "  Type: article section\n",
      "  Path: PART 27 Other Commissions > 259. Functions, duties and powers of National Inclusion Commission\n",
      "  Vector dimension: 1\n",
      "  Vector (first 5 values): ['default']\n",
      "\n",
      "Chunk 91 (Page 55):\n",
      "  Type: section\n",
      "  Path: PART -11 Judiciary > 128. Supreme Court > 129. Appointment and qualifications of Chief Justice and Justices of the Supreme Court\n",
      "  Vector dimension: 1\n",
      "  Vector (first 5 values): ['default']\n",
      "\n",
      "Chunk 196 (Page 116):\n",
      "  Type: section\n",
      "  Path: PART 27 Other Commissions > 256. Functions, duties and powers of National Dalit Commission\n",
      "  Vector dimension: 1\n",
      "  Vector (first 5 values): ['default']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify upload - check total count and sample documents\n",
    "documents = client.collections.get(\"Document\")\n",
    "\n",
    "# Get actual count using aggregate\n",
    "count = documents.aggregate.over_all(total_count=True).total_count\n",
    "print(f\"Total documents in Weaviate: {count}\\n\")\n",
    "\n",
    "# Fetch samples WITH vectors\n",
    "response = documents.query.fetch_objects(limit=3, include_vector=True)\n",
    "print(\"Sample documents:\")\n",
    "for obj in response.objects:\n",
    "    props = obj.properties\n",
    "    print(f\"Chunk {props['chunk_id']} (Page {props['page_number']}):\")\n",
    "    print(f\"  Type: {props['chunk_type']}\")\n",
    "    print(f\"  Path: {props['breadcrumb']}\")\n",
    "    \n",
    "    # Vector handling\n",
    "    if obj.vector:\n",
    "        vector_list = list(obj.vector) if not isinstance(obj.vector, list) else obj.vector\n",
    "        print(f\"  Vector dimension: {len(vector_list)}\")\n",
    "        print(f\"  Vector (first 5 values): {vector_list[:5]}\")\n",
    "    else:\n",
    "        print(f\"  Vector: Not included\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted collection: Document\n",
      "All collections deleted. Database is now empty.\n"
     ]
    }
   ],
   "source": [
    "# # Delete all collections from Weaviate database\n",
    "# collections = client.collections.list_all()\n",
    "# for collection_name in collections:\n",
    "#     try:\n",
    "#         client.collections.delete(collection_name)\n",
    "#         print(f\"Deleted collection: {collection_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error deleting {collection_name}: {e}\")\n",
    "\n",
    "# print(\"All collections deleted. Database is now empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-rag (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
